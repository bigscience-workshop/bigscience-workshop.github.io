# Philosophy and short history

## Philosophy

The acceleration in Artificial Intelligence (AI) and Natural Language Processing (NLP) will have a **fundamental impact on society**, as these technologies are at the core of the tools we use on a daily basis. A considerable part of this effort currently stems in NLP from training increasingly larger language models on increasingly larger quantities of texts.

Unfortunately, the resources necessary to create the best-performing models are found mainly at big technology giants. The stranglehold on this transformative technology poses some problems, from a research advancement, environmental, ethical and societal perspective.

For example, while recent models such as GPT3 (from OpenAI / Microsoft) shows interesting behaviors from a research point of view, such models are private and not accessible to many academic organizations. Moreover, even when accessible, these tools have not been designed as research artifacts and for instance, lack access to the training dataset or checkpoints which makes it impossible to answer many important research questions around these models (capabilities, limitations, potential improvements, bias, ethics, environmental impact, general AI/cognitive research landscape). The current situation also promotes a duplication of energy requirements and environmental costs, due to the duplicated training of large models in private settings. Finally, these models are usually anglo-centric and there are‡ shortcomings in the text corpora used to train these models, ranging from non-representativeness of populations to a predominance of potentially harmful stereotypes or the inclusion of personally-identifying information.

The BigScience project aims to demonstrate another way of creating, studying, and sharing large language models and large research artifacts in general within the AI/NLP research communities.

This project takes inspiration from scientific creation schemes existing in other scientific fields, such as **CERN and the LHC in particle physics**, in which **open scientific collaborations** facilitate the creation of **large-scale artifacts useful for the entire research community**.

Gathering a much larger research community around the creation of these artifacts makes it possible to consider in advance the many **research questions surrounding large language models** (capabilities, limitations, potential improvements, bias, ethics, environmental impact, general AI/cognitive research landscape) that will be interesting to answer with the created artifacts and to reflect and prepare the tools needed to answer as many of these questions as possible.

The BigScience open-science project is seen as a proposal for an international and inclusive way of performing collaborative research. Beyond the research artifacts created and shared, this project thus aims to bring together all the skills, conditions, and lessons allowing such future experiments of large-scale scientific collaboration.

In the end, it’s thus the deep belief of the founding members that the project’s success will ultimately be measured by its long-term impact on the field of NLP and AI: by proposing **an alternative way to conduct large scale science projects**.

## Short history

The “BigScience” project originated from discussions in early 2021 between Thomas Wolf (HuggingFace), Stéphane Requena and Pierre-François Lavallee (respectively from GENCI and IDRIS, both institutions being behind the [French supercomputer Jean Zay](http://www.idris.fr/eng/jean-zay/jean-zay-presentation-eng.html)). Very quickly, members of the science team of HuggingFace (Victor Sanh, Yacine Jernite and team) as well as members of the French academic and industrial AI and NLP research communities joined the discussions to further develop the project leading the grant application for 5 millions hours in February 2021.

About Jean Zay: During the first half of 2019, the Jean Zay computer was installed at IDRIS, national computing centre for the CNRS (Centre national de la recherche scientifique), with an impressive performance of 15.9 Pflop/s (15.9 million billion floating point operations per second). With an extension during 2020 the peak performance of 28 Pflops/s was achieved, opening new possibilites to extend beyond the classic usage modes of high performance computing to new usages in artificial intelligence. 

![](uploads/images/jean-zay_copyright_phototheque_CNRS_cyril_fresillon.jpeg)

- Image source/credits: [IDRIS Jean Zay](http://www.idris.fr/eng/jean-zay/jean-zay-presentation-eng.html)

While BigScience quickly gathered a large French research community surrounding the public compute facility "Jean Zay", ranging from academic laboratories to startups/SMEs and larger industrial groups (identified as the founding members in the members page), it was also clear from the inception of the project that, to propose a more inclusive way to conduct open-research on these large scale artifacts, the international community should be included as soon as possibler and the project should be open and welcome the participants from a large and diverse set of nationalities, cultures, origins and research fields. Following the early grant submission, the funding members of the BigScience project thus started to open and extend the project to an international research community interested in studying and understand better the many research questions surrounding large language models as well as the challenges around creating and sharing such models and datasets for research purposes.

When the project reached more than 200 participants, the organization of the project started to take shape and it was decided to adopt the structure of a research workshop which appeared as one of the most inclusive, flexible and research-community-focused strucutre for research collaboration.

The project is thus now defined as a one-year long research workshop with a set of collaborative tasks around the creation of a large multilingual dataset and a large multilingual language model as detailed on the main page and in the organization documents as detailed on the [main page](index.md) on in the [organization documents](pages/resources.md).

## Founding members

Hereafter are the founding members who participated in the writing of the [original application](https://drive.google.com/file/d/14cagP3ikBCaAvcmTw2xFrol7-sphQlLA/view?usp=sharing) submitted to the to the Jean Zay supercomputer.

### Thomas Wolf, Victor Sanh, Yacine Jernite (HuggingFace - open-science team)
Hugging Face, at the inception of the BigScience project, develops open-source research tools that are widely used in the NLP community.
Stéphane Requena (GENCI), Pierre-François Lavallée (IDRIS)
GENCI is the architect of the Jean Zay supercomputing facility. The IDRIS is the major centre of very high performance intensive numerical computation for the French National Centre for Scientific Research (CNRS) and operates several supercomputing facilities in France, including the Jean Zay supercomputer.

### Alexandre Allauzen (Professor)
ESPCI and LAMSADE (Dauphine Université, PSL, CNRS)
I am a member of the MILES team (Machine Intelligence and Learning Systems) and my research focuses on the development of neural models for NLP and  their robustness. I worked on language modelling along with generative models for sequences, learning criterion for large vocabulary applications, and dynamical models. 

### Farah Benamara (Associate professor), Chloé Braud (Researcher), Philippe Muller (Associate professor), Véronique Moriceau (Associate professor)
MELODI team at IRIT/University of Toulouse
Our team focuses on semantics and pragmatics, including discourse parsing, sentiment analysis, and bias detection. Contextual embeddings are used mostly for fine-tuning models, and studies on interpretability.

### Vincent Claveau (Researcher), Antoine Chaffin (PhD student)
IRISA - LinkMedia team - IMATAG/CNRS
Our group develops research in NLP and multimodal analysis (eg. text/image). The expected  outcomes of this project would be very valuable for LinkMedia's current research topics relying on the use of artificially generated content: detection of (textual  and multimodal) fake news, artificial datasets for classification or tagging and Information Retrieval.

### Mathieu Constant (Full professor)
Université de Lorraine, ATILF - UMR 7118 - CNRS / UL
Our team is interested in resources, normalization, annotation and exploitation in NLP.

### Benoît Crabbé (Professor), Marie Candito (Assistant professor), Antoine Simoulin (Phd Student) and members of the LLF
University of Paris
The Laboratoire de Linguistique Formelle is a member of the Labex EFL - Empirical Foundations of Linguistics. The team studies all aspects of language. We participated in the release of the FlauBERT model and adapted the now-famous GPT-2 model in French. The paper is currently under submission and the model, which counts over 1 billion parameters, will be released in open-source upon acceptance.

### Béatrice Daille (Full professor)
GdR TAL (CNRS)
The GDR TAL is the French academic hub for researchers in NLP. It is interested in language in all its forms: written, oral, signed. It deals with the themes of computer modeling and machine learning of language, its manifestations and applications in society, including their ethics issues.

### Peter Ford Dominey (Research director)
CNRS DR1, INSERM UMR1093, UBFC, Dijon
Language encoding models help explain language processing in the human brain by learning functions that predict brain responses from the language stimuli that elicited them (Jain & Huth 2018 BioRxiv). The proposed project should lead to a significant advance in the quality of language models that we can use to explain brain responses in humans from the same stimuli. Relevant publication: Uchida, T., Lair, N., Ishiguro, H., & Dominey, P. F. (2021) Neurobiology of Language, 2(1), 83-105.

### Benoît Favre (Associate professor), Frederic Bechet (Professor)
Aix-Marseille University, UTLN, CNRS LIS/UMR7220
Benoît represents the multimodality and intermodality work group at GdR TAL, the CNRS research group on natural language processing. He is interested in the generalization properties of machine learning models and in developing appropriate methodology for studying NLP systems.

### Bertrand Delezoide (Head of the laboratory), Olivier Ferret (Professor), Adrian Popescu (Senior researcher), Julien Tourille (Researcher)
CEA LASTI
The LASTI team focuses on the extraction, classification and semantic analysis of both textual and image content with application to: technological watch, classical and social media analysis, user privacy preservation, clinical information extraction. The model targeted in this project will be useful for our research on medical document generation, text summarization, data augmentation, prompting tasks for building semantic resources and bias mitigation in texts.

### Karën Fort (Associate professor)
Sorbonne Université / LORIA
I am interested in the multilingual dataset creation and the ethical and legal aspects of the project. My research interests are language resources and ethics in NLP and AI.

### Claire Gardent (Research director) and Christophe Cerisara (Researcher)
CNRS, LORIA UMR7503, Nancy
We are computational linguists currently working mostly on Natural Language Generation (NLG) and Human-Machine Dialog. Claire holds an AI Chair (XNLG) on multilingual, multisource NLG. We plan to investigate (i) how the pre-trained model proposed by the project can be used to support generation from knowledge-bases, text and meaning representations into multiple languages and (ii) whether it supports factually correct NLG (e.g. do the enriched prompts provide a better support to ensure a correct match between input and generated text?).
Relevant publications: https://www.aclweb.org/anthology/2020.emnlp-main.231.pdf  https://www.aclweb.org/anthology/D19-1428.pdf 

### Céline Hudelot (Assistant professor)
MAS Laboratory (Applied Mathematics and Systems research laboratory) of Ecole Centrale Paris
I lead the research axis on formal methods for semantic multimedia understanding in the LOGIMAS Team.

### Joseph Le Roux (Associate professor) and Nadi Tomeh (Assistant professor)
RCLN/LIPN, UMR 7030 Univ. Sorbonne-Paris-Nord/CNRS
The RCLN/LIPN group is part of the LABEX EFL - Empirical Foundation of Linguistics. The team focuses on the computational aspects of NLP, especially in multilingual controlled generation. The model targeted by this project would be an asset to conduct our research.

### Antoine Neuraz (Assistant professor) and Ivan Lerner (PhD student)
Université de Paris and at Necker - Enfants Malades hospital
Antoine is an assistant professor in biomedical informatics, and a member of the INSERM research team "Information Sciences to support Personalized Medicine" (Pr. A. Burgun). His work focuses on the secondary use of healthcare data for research and machine learning, in particular through the use of natural language processing to facilitate information retrieval and access to information in computerized patient records.

### Aurélie Névéol (Researcher), Anne-Laure Ligozat (Associate professor), Caio Corro (Associate professor), François Yvon (Research director) and students affiliated to LISN
Université Paris Saclay, LISN, CNRS UMR9105
We are notably interested in machine translation, multilingual NLP, cross-lingual transfer machine learning techniques, and responsible NLP. The proposed model of this project could be used to investigate language model bias as well as to create shareable synthetic datasets in specialized domains which is one of the objectives of the ANR funded CODEINE project.

### Pierre-Yves Oudeyer (Research director), Cédric Colas (PhD student), Grgur Kovac (Research engineer), Tristan Karch (PhD student) and the Flowers project-team
Inria, Univ. Bordeaux and Ensta ParisTech
The Flowers team studies models of open-ended development and learning to help us understand better how children learn, and build machines that learn like children. A key topic is language grounding: how can machines learn and use language as a tool to communicate and achieve joint tasks with humans. The large models targeted in this project would be a major asset towards enabling powerful goal imagination and the ability for such systems to be driven by natural language input of non-specialist users (Colas et al., 2020). We also aim to leverage such large models in educational projects where they can be used as tools to foster curiosity (Oudeyer et al., 2016) and learn skills such as question asking (Alaimi et al., 2020) or creative writing.

### Benoît Sagot (Research director), Djamé Seddah (Tenured associate professor), Pedro Ortiz (PhD student) and the ALMAnaCH project-team
Inria Paris
The project-team focuses on the syntactic and semantic parsing of natural languages, including noisy web-based data, using symbolic, statistical, neural and hybrid techniques. One of the most recent challenges is the integration of contextual information (linguistic and non-linguistic). The team is also involved in Digital and Computational Humanities, especially the study and modelling of linguistic variation, while participating in the development of text simplification tools. The team has recently released major resources for the community, including the CamemBERT neural language model for French and the large-scale multilingual corpus OSCAR.

### Ludovic Tanguy (Associate professor), and the NLP team of the CLLE lab
Univ. Toulouse - CNRS
As computational and corpus linguists we have investigated various generations of distributional semantics models, questioning their use in descriptive linguistics and the intricate relations between training corpora and the resulting models. We need such joint research effort to be able to use large corpora and language models in our work, and thus provide a link between these new artefacts and linguistic research.

### Xavier Tannier (Full professor)
Sorbonne Université, LIMICS (Sorbonne Université, Inserm, Univ. Sorbonne Paris Nord)
My research topics concern natural language processing and information retrieval and extraction, mostly (but not only) in the field of biomedical applications. In my team, we commonly use pre-trained models to transfer or fine-tune them in specific applications. The multilingual nature of the models is of considerable importance for our applications on French health documents.

### Serena Villata (Tenured researcher), Elena Cabrio (Assistant professor), and members of the I3S Laboratory
I3S Laboratory, CNRS, INRIA, Université Côte d’Azur
Our research focuses on the topic of argument(ation) mining, with the aim to detect and generate textual argumentative structures in different domains ranging from healthcare documents to political debates to online discussions. We also tackle the issue of online hate speech detection and classification in a multilingual context. The outcomes of this research project are valuable for our research topics because they can help improve the classification of online hate speech instances where multilingualism is particularly relevant, and tackling the open challenge of generating argumentative structures to be employed in argument-based explanatory dialogues.

### Guillaume Alleon (Head of AI research), Alexandre Arnold (Senior researcher), Catherine Kobus (Senior research engineer), Francois Lancelot (Software Engineer)
Airbus, Central Research & Technology
Airbus has been experimenting with state-of-the-art language models along 2 directions: A/ using pre-trained language models as part of bigger systems for instance to build vocal assistants that accurately answer questions on pilots’ documentation; B/ training language models from scratch on domain-specific data. The model targeted in this project is interesting to us in two aspects: more powerful language models are crucial to transform prototypes into actual services especially when domain specific knowledge is required and handling languages beyond English is critical as Airbus operates in all regions of the world.
Relevant publication: https://arxiv.org/abs/2011.13284

### Jean-Michel Dussoux (Technical and innovation director)
Cloud Temple
Cloud Temple has developed a dialogue agent that is able to identify and learn new skills through open-domain conversation with the user without specification of these skills beforehand \citep{Lair20}. While fine-tuning pre-trained models such as BERT offers a solid approach, the zero-shot capabilities of GPT3 allow it to address new challenges such as recognizing the intent of the user by conditioning on a linearized knowledge graph. The model targeted in this project is interesting to us in particular because of its focus on multilingualism (covering French in a non-trivial way) and its ability to handle very long conditionings. 

### Robert Vesoul (CEO, Co-founder at Illuin Technology & Co-Director of the Digital Innovation Chair @CentraleSupelec), Gautier Viaud (Lead data scientist), Martin d'Hoffschmidt (Lead data scientist), Wacim Belblidia (Lead data scientist)
Illuin Technology
ILLUIN Technology is a team of tech makers tackling the challenges of artificial intelligence and how it enables new means of user interaction.

### Romain Riviere (CTO)
Levia.ai
Levia.ai is focused on developing highly-personalized, one-to-one shopping experiences creating a unique virtual vendor powered by AI. Its mission is to build the most effortless, customized, and curated shopping experience possible by putting conversations at the heart of your conversion strategy.

### Igor Carron (CEO), Laurent Daudet (CTO), Iacopo Poli (ML Group Lead), and Julien Launay, (Extreme-Scale Project Lead)
LightOn 
LightOn develops photonic chips for training and running the largest ML models of tomorrow. We cooperate with academic and industrial partners through research collaborations, and by providing access to our photonic co-processors through LightOn Cloud and LightOn Appliance. Extreme-scale NLP models open exciting research directions as well as new business opportunities. However, current trends in model scaling are unsustainable. Hardware innovation, such as the one undertaken at LightOn, is key to make large models more scalable and affordable. Our philosophy is to develop computational hardware at the speed of software. As such, white-box access to a state-of-the-art extreme-scale model, and how it is trained, is an unprecedented opportunity for us to better inform the development of our future hardware. 

### Alexandre Lebrun (CEO), Martin Raison (CTO), Samuel Humeau (Machine learning engineer)
Nabla
Nabla develops medical software aiming at simplifying the task of healthcare professionals. Observing large inefficiencies in the current healthcare system (patients waiting for the last moment to see a doctor, healthcare professionals wasting a lot of time on bureaucratic tasks), Nabla aims to build software that brings people and healthcare professionals closer, and automatically format health records for doctors. The latter task requires a deep understanding of medical material (such as a discussion between a patient and a healthcare professional), which can only be achieved with the crafting of larger, more powerful text analysis models.

### Matthias Gallé (Lab Manager) and Laurent Besacier (NLP group lead)
Naver Labs Europe
Naver Labs Europe is the largest industrial AI research lab in France. Our research on controlled natural language generation would benefit tremendously from access to a white-box alternative of a large language model and would allow controlling mechanisms out-of-scope with current closed-sourced models. The lack of such access is restricting potential services such as summarization, data-to-text and question answering. Another research direction is to investigate more deeply the zero and few shot capacities of these models for machine translation.
Relevant publications: https://arxiv.org/abs/2012.11635, https://www.aclweb.org/anthology/2020.nlpcovid19-2.16/, https://arxiv.org/abs/2004.14754

### Géraldine Damnati (Research engineer), Johannes Heinecke (Researcher), Frederic Herledan (Team lead)
Orange Labs
Orange is one of the world’s leading telecommunications operators.

### Jean-Louis Queguiner (Head of Artificial Intelligence and Data), Guillaume Salou (Team Lead Machine Learning Services)
OVHcloud
OVHcloud is the European leader of cloud computing and the 3rd largest web hosting provider in the world. It defends an innovative and different cloud, which respects the European values of freedom of choice, transparency, openness to standards and protection of privacy.

### Thomas Scialom (Partner and researcher at LIP6), Gilles Moyse (Co-founder/CEO), Jacopo Staiano (Research lead)
ReciTAL
ReciTAL is a French leader in NLP. My research focuses on natural language generation (NLG) and its automatic evaluation. Language Models have enabled significant progress in the generation of English texts. However, very few works have tested their multilingual ability for generation. It will be interesting to explore how prompts engineering allows to control the generation language, through different tasks such as question generation or automatic summarization.
Relevant publications:
https://www.aclweb.org/anthology/2020.emnlp-main.647/, https://papers.nips.cc/paper/2020/hash/db261d4f615f0e982983be499e57ccda-Abstract.html, https://www.aclweb.org/anthology/2020.lrec-1.673/

### Vincent Feuillard (Statistical Learning & Data Science Expert), Joan André (Data Science & Statistics Manager), François-Paul Servant (Senior NLP researcher), Raphael Sourty (NLP Phd), Ayhan Uyanik (Senior Data Scientist)
Renault Group, Research & Quality and Customer Satisfaction Department
Since 2015, Group Renault has been studying Text Mining and particularly Natural Language Processing to analyze and understand customer’s feedback coming from various textual sources (call centers, surveys …). They are highly valuable to understand the influence factors of customer satisfaction. We worked on topics such as categorization or search engines to automate and simplify human readings. To do so, we have tested pre-trained language models approaches fine-tuned on our specific corpus and tasks, using among other libraries, transformers. More widely, Renault not only produces cars but also generates a lot of textual data in many languages with a very specific vocabulary. To embed this vocabulary and more generally to embed the associated knowledge, some research activities are currently done in collaboration with IRIT (PhD of R. Soutry).
Training huge language models is unfortunately very expensive and time consuming. We are then interested in participating in this new NLP research initiative.

### Jean Senellart (CEO), Josep Crego (Head of research), Elise Michon (Research engineer), Guillaume Klein (Research engineer), Dakun Zhang (R&D engineer), Natalia Segal (R&D engineer)
SYSTRAN
SYSTRAN is a 50-year old pure player in machine translation technology and is cofounder and main developer of OpenNMT framework promoting open, industry-ready framework for machine translation currently used and cited in over 2000 academic papers since 2017. SYSTRAN is offering tools to build neural models and a marketplace for neural machine translation models allowing experts around the world to leverage their expertise and proprietary data without losing their IP. SYSTRAN has an active research activity on domain adaptation, priming techniques for machine translation, data cleaning, but also on extension to sign language and open-source NLP.
Relevant publications:

### KLEIN, Guillaume, KIM, Yoon, DENG, Yuntian, et al. Opennmt: Open-source toolkit for neural machine translation. arXiv preprint arXiv:1701.02810, 2017.
SENELLART, Jean, ZHANG, Dakun, WANG, Bo, et al. OpenNMT system description for WNMT 2018: 800 words/sec on a single-core CPU. In : Proceedings of the 2nd Workshop on Neural Machine Translation and Generation. 2018. p. 122-128.
PHAM, Minh Quang, CREGO, Josep M., SENELLART, Jean, et al. Fixing translation divergences in parallel corpora for neural mt. In : Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 2018. p. 2967-2973.

### Guillaume Gaudron (Director development @ "Ubisoft La Forge")
Ubisoft
Guillaume is building "La Forge France", the French research center of Ubisoft.

## Full members list

After the insception of the project, the list of members has grown significantly and can be seen [here](pages/participants.md).
